{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "rcQsEcxgCwpp",
        "outputId": "f33c556c-d0c8-48b9-c1dc-72387e835781"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1feedc2bc197>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KAnRwb3DC9Ib",
        "outputId": "267e25cd-0b2f-481d-ae8e-1b1285ee329b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "\n",
        "# --------------------------\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# Simple Data Augmentation: Random horizontal and vertical flips\n",
        "def simple_augmentation(x):\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip (width axis)\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip (height axis)\n",
        "    return x\n",
        "\n",
        "# --------------------------\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "            print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "            print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply z-score normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "        sample = (sample - np.mean(sample)) / (np.std(sample) + 1e-5)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# --------------------------\n",
        "# Define Squeeze-and-Excitation (SE) Block\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "# --------------------------\n",
        "# Build a Physics-Informed Model: ResNet18 with SE blocks and additional FC layers\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class ResNetSE_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNetSE_PDE, self).__init__()\n",
        "        # Load ResNet18 backbone\n",
        "        self.base = resnet18(pretrained=False)\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject SE blocks in the first block of each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], SEBlock(64))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], SEBlock(128))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], SEBlock(256))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], SEBlock(512))\n",
        "\n",
        "        # Additional fully connected layers to deepen the classifier\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.base.conv1(x)\n",
        "        x = self.base.bn1(x)\n",
        "        x = self.base.relu(x)\n",
        "        x = self.base.layer1(x)\n",
        "        x = self.base.layer2(x)\n",
        "        x = self.base.layer3(x)\n",
        "        x = self.base.layer4(x)   # Extract convolutional features here\n",
        "        features = x             # Save features for PDE regularization\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc_layers(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# --------------------------\n",
        "# Focal Loss Implementation\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# --------------------------\n",
        "# PDE Regularization: Laplacian-Based Smoothness Constraint on Features\n",
        "def pde_regularization(features):\n",
        "    \"\"\"\n",
        "    Computes a PDE-inspired regularization loss by calculating the\n",
        "    Laplacian of the feature maps and penalizing high-frequency variations.\n",
        "    \"\"\"\n",
        "    N, C, H, W = features.shape\n",
        "    lap_kernel = torch.tensor([[0, 1, 0],\n",
        "                               [1, -4, 1],\n",
        "                               [0, 1, 0]], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel = lap_kernel.view(1, 1, 3, 3).repeat(C, 1, 1, 1)\n",
        "    laplacian = F.conv2d(features, lap_kernel, padding=1, groups=C)\n",
        "    loss_pde = torch.mean(laplacian ** 2)\n",
        "    return loss_pde\n",
        "\n",
        "# --------------------------\n",
        "# File paths (update paths to files in /content)\n",
        "photon_file = \"/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1 (1).hdf5\"\n",
        "electron_file = \"/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "# Create dataset and apply augmentation\n",
        "dataset = ParticleDataset(photon_file, electron_file, transform=simple_augmentation)\n",
        "\n",
        "# Split dataset: 80% train, 20% test\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size],\n",
        "                                             generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------\n",
        "# Initialize Model, Loss, Optimizer, and Scheduler\n",
        "model = ResNetSE_PDE(num_classes=2).to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = FocalLoss(gamma=2, alpha=0.25)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "steps_per_epoch = len(train_loader)\n",
        "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=steps_per_epoch, epochs=20)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# Set PDE regularization weight (tune this parameter as needed)\n",
        "lambda_pde = 0.01\n",
        "\n",
        "# --------------------------\n",
        "# Training Loop with PDE Regularization\n",
        "num_epochs = 20\n",
        "train_losses, train_accs = [], []\n",
        "test_losses, test_accs = [], []\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for inputs, labels in train_iter:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits, features = model(inputs, return_features=True)\n",
        "            cls_loss = criterion(logits, labels)\n",
        "            reg_loss = pde_regularization(features)\n",
        "            loss = cls_loss + lambda_pde * reg_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        train_iter.set_postfix({\"loss\": running_loss/total, \"acc\": 100.*correct/total})\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100. * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_iter = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
        "        for inputs, labels in test_iter:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                logits, features = model(inputs, return_features=True)\n",
        "                cls_loss = criterion(logits, labels)\n",
        "                reg_loss = pde_regularization(features)\n",
        "                loss = cls_loss + lambda_pde * reg_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            all_preds.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            test_iter.set_postfix({\"loss\": test_loss/total, \"acc\": 100.*correct/total})\n",
        "\n",
        "    test_loss = test_loss / total\n",
        "    test_acc = 100. * correct / total\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), \"resnetse_pde_best.pth\")\n",
        "        print(f\"New best model saved with accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "# --------------------------\n",
        "# Evaluation: ROC Curve and Classification Report\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('roc_curve.png')\n",
        "plt.show()\n",
        "\n",
        "y_pred_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
        "print(classification_report(all_labels, y_pred_bin))\n",
        "\n",
        "# --------------------------\n",
        "# Plot Training Curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curves')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(test_accs, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best model accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "# --------------------------\n",
        "# Save Final Model (optional: update path if needed)\n",
        "model_save_path = \"/content/resnetse_pde_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to: {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "QpnJJi_ODHF4",
        "outputId": "12dacd92-362d-4f8f-ff6a-3b50d7175b8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Unable to synchronously open file (truncated file: eof = 98566144, sblock->base_addr = 0, stored_eof = 119703858)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-14deafbddd8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;31m# Create dataset and apply augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParticleDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoton_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melectron_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimple_augmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;31m# Split dataset: 80% train, 20% test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-14deafbddd8d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, photon_file, electron_file, transform)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Read photon dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoton_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_photon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_photon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoton_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_photon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (truncated file: eof = 98566144, sblock->base_addr = 0, stored_eof = 119703858)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "# Attempt to open the HDF5 file in read-only mode\n",
        "try:\n",
        "    with h5py.File(photon_file, 'r') as f:\n",
        "        # If successful, print the keys within the HDF5 file\n",
        "        print(\"Keys in HDF5 file:\", list(f.keys()))\n",
        "except OSError as e:\n",
        "    # If an error occurs, print the error message\n",
        "    print(\"Error opening HDF5 file:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rQ7uJ2KTDSpi",
        "outputId": "d7d6a460-f867-4d9d-abb3-0713f344cb49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error opening HDF5 file: Unable to synchronously open file (truncated file: eof = 115343360, sblock->base_addr = 0, stored_eof = 119703858)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# --------------------------\n",
        "# Download files from CERNBox if they are not available locally\n",
        "\n",
        "photon_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "electron_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "photon_file = \"/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "electron_file = \"/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "if not os.path.exists(photon_file):\n",
        "    print(f\"Downloading {photon_file} ...\")\n",
        "    urllib.request.urlretrieve(photon_url, photon_file)\n",
        "else:\n",
        "    print(f\"File {photon_file} already exists.\")\n",
        "\n",
        "if not os.path.exists(electron_file):\n",
        "    print(f\"Downloading {electron_file} ...\")\n",
        "    urllib.request.urlretrieve(electron_url, electron_file)\n",
        "else:\n",
        "    print(f\"File {electron_file} already exists.\")\n",
        "\n",
        "# --------------------------\n",
        "# The remaining code: advanced physics-informed network with PDE regularization\n",
        "\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# Simple Data Augmentation: Random horizontal and vertical flips\n",
        "def simple_augmentation(x):\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip (width axis)\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip (height axis)\n",
        "    return x\n",
        "\n",
        "# --------------------------\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "            print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "            print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply z-score normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "        sample = (sample - np.mean(sample)) / (np.std(sample) + 1e-5)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# --------------------------\n",
        "# Define Squeeze-and-Excitation (SE) Block\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "# --------------------------\n",
        "# Build a Physics-Informed Model: ResNet18 with SE blocks and additional FC layers\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class ResNetSE_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNetSE_PDE, self).__init__()\n",
        "        # Load ResNet18 backbone\n",
        "        self.base = resnet18(pretrained=False)\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject SE blocks in the first block of each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], SEBlock(64))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], SEBlock(128))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], SEBlock(256))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], SEBlock(512))\n",
        "\n",
        "        # Additional fully connected layers to deepen the classifier\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.base.conv1(x)\n",
        "        x = self.base.bn1(x)\n",
        "        x = self.base.relu(x)\n",
        "        x = self.base.layer1(x)\n",
        "        x = self.base.layer2(x)\n",
        "        x = self.base.layer3(x)\n",
        "        x = self.base.layer4(x)   # Extract convolutional features here\n",
        "        features = x             # Save features for PDE regularization\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc_layers(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# --------------------------\n",
        "# Focal Loss Implementation\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# --------------------------\n",
        "# PDE Regularization: Laplacian-Based Smoothness Constraint on Features\n",
        "def pde_regularization(features):\n",
        "    \"\"\"\n",
        "    Computes a PDE-inspired regularization loss by calculating the\n",
        "    Laplacian of the feature maps and penalizing high-frequency variations.\n",
        "    \"\"\"\n",
        "    N, C, H, W = features.shape\n",
        "    lap_kernel = torch.tensor([[0, 1, 0],\n",
        "                               [1, -4, 1],\n",
        "                               [0, 1, 0]], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel = lap_kernel.view(1, 1, 3, 3).repeat(C, 1, 1, 1)\n",
        "    laplacian = F.conv2d(features, lap_kernel, padding=1, groups=C)\n",
        "    loss_pde = torch.mean(laplacian ** 2)\n",
        "    return loss_pde\n",
        "\n",
        "# --------------------------\n",
        "# Create Dataset using the downloaded files\n",
        "dataset = ParticleDataset(photon_file, electron_file, transform=simple_augmentation)\n",
        "\n",
        "# Split dataset: 80% train, 20% test\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size],\n",
        "                                             generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------\n",
        "# Initialize Model, Loss, Optimizer, and Scheduler\n",
        "model = ResNetSE_PDE(num_classes=2).to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = FocalLoss(gamma=2, alpha=0.25)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "steps_per_epoch = len(train_loader)\n",
        "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=steps_per_epoch, epochs=20)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# Set PDE regularization weight (tune this parameter as needed)\n",
        "lambda_pde = 0.01\n",
        "\n",
        "# --------------------------\n",
        "# Training Loop with PDE Regularization\n",
        "num_epochs = 20\n",
        "train_losses, train_accs = [], []\n",
        "test_losses, test_accs = [], []\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for inputs, labels in train_iter:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits, features = model(inputs, return_features=True)\n",
        "            cls_loss = criterion(logits, labels)\n",
        "            reg_loss = pde_regularization(features)\n",
        "            loss = cls_loss + lambda_pde * reg_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        train_iter.set_postfix({\"loss\": running_loss/total, \"acc\": 100.*correct/total})\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100. * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_iter = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\")\n",
        "        for inputs, labels in test_iter:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                logits, features = model(inputs, return_features=True)\n",
        "                cls_loss = criterion(logits, labels)\n",
        "                reg_loss = pde_regularization(features)\n",
        "                loss = cls_loss + lambda_pde * reg_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            all_preds.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            test_iter.set_postfix({\"loss\": test_loss/total, \"acc\": 100.*correct/total})\n",
        "\n",
        "    test_loss = test_loss / total\n",
        "    test_acc = 100. * correct / total\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), \"resnetse_pde_best.pth\")\n",
        "        print(f\"New best model saved with accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "# --------------------------\n",
        "# Evaluation: ROC Curve and Classification Report\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('roc_curve.png')\n",
        "plt.show()\n",
        "\n",
        "y_pred_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
        "print(classification_report(all_labels, y_pred_bin))\n",
        "\n",
        "# --------------------------\n",
        "# Plot Training Curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curves')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(test_accs, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best model accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "# --------------------------\n",
        "# Save Final Model (optional: update path if needed)\n",
        "model_save_path = \"/content/resnetse_pde_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to: {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4868
        },
        "id": "gOOYhDnbESmR",
        "outputId": "27eada3a-908d-47d1-afce-4625cae18920"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading /content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5 ...\n",
            "Downloading /content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5 ...\n",
            "Using device: cuda\n",
            "Loaded photon data: (249000, 32, 32, 2)\n",
            "Loaded electron data: (249000, 32, 32, 2)\n",
            "Total samples: 498000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-1-9dcba58fa6f3>:232: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNetSE_PDE(\n",
            "  (base): ResNet(\n",
            "    (conv1): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): Identity()\n",
            "    (layer1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): SEBlock(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Linear(in_features=64, out_features=4, bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Linear(in_features=4, out_features=64, bias=False)\n",
            "            (3): Sigmoid()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): SEBlock(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Linear(in_features=128, out_features=8, bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Linear(in_features=8, out_features=128, bias=False)\n",
            "            (3): Sigmoid()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): SEBlock(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Linear(in_features=256, out_features=16, bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Linear(in_features=16, out_features=256, bias=False)\n",
            "            (3): Sigmoid()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): SEBlock(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=32, bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Linear(in_features=32, out_features=512, bias=False)\n",
            "            (3): Sigmoid()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 [Train]:   0%|          | 0/6225 [00:00<?, ?it/s]<ipython-input-1-9dcba58fa6f3>:255: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "Epoch 1/20 [Train]: 100%|██████████| 6225/6225 [04:14<00:00, 24.50it/s, loss=0.0413, acc=62.5]\n",
            "Epoch 1/20 [Test]:   0%|          | 0/1557 [00:00<?, ?it/s]<ipython-input-1-9dcba58fa6f3>:289: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "Epoch 1/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 52.24it/s, loss=0.0395, acc=66.1]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20, Train Loss: 0.0413, Train Acc: 62.52%, Test Loss: 0.0395, Test Acc: 66.14%\n",
            "New best model saved with accuracy: 66.14%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 6225/6225 [04:16<00:00, 24.31it/s, loss=0.0392, acc=66.5]\n",
            "Epoch 2/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 52.09it/s, loss=0.0394, acc=65.9]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/20, Train Loss: 0.0392, Train Acc: 66.47%, Test Loss: 0.0394, Test Acc: 65.94%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 6225/6225 [04:20<00:00, 23.94it/s, loss=0.039, acc=67.1]\n",
            "Epoch 3/20 [Test]: 100%|██████████| 1557/1557 [00:30<00:00, 51.74it/s, loss=0.041, acc=61.1]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/20, Train Loss: 0.0390, Train Acc: 67.10%, Test Loss: 0.0410, Test Acc: 61.05%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 6225/6225 [04:18<00:00, 24.11it/s, loss=0.0392, acc=67]\n",
            "Epoch 4/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 52.86it/s, loss=0.0387, acc=67.6]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/20, Train Loss: 0.0392, Train Acc: 67.05%, Test Loss: 0.0387, Test Acc: 67.63%\n",
            "New best model saved with accuracy: 67.63%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 6225/6225 [04:12<00:00, 24.62it/s, loss=0.0394, acc=66.7]\n",
            "Epoch 5/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 52.02it/s, loss=0.0427, acc=58]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/20, Train Loss: 0.0394, Train Acc: 66.70%, Test Loss: 0.0427, Test Acc: 58.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 6225/6225 [04:15<00:00, 24.39it/s, loss=0.0396, acc=66.4]\n",
            "Epoch 6/20 [Test]: 100%|██████████| 1557/1557 [00:28<00:00, 54.93it/s, loss=0.0435, acc=57.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20, Train Loss: 0.0396, Train Acc: 66.36%, Test Loss: 0.0435, Test Acc: 57.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 6225/6225 [04:08<00:00, 25.06it/s, loss=0.0397, acc=66.4]\n",
            "Epoch 7/20 [Test]: 100%|██████████| 1557/1557 [00:30<00:00, 51.55it/s, loss=0.0388, acc=68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20, Train Loss: 0.0397, Train Acc: 66.39%, Test Loss: 0.0388, Test Acc: 67.98%\n",
            "New best model saved with accuracy: 67.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 6225/6225 [04:12<00:00, 24.66it/s, loss=0.0396, acc=66.5]\n",
            "Epoch 8/20 [Test]: 100%|██████████| 1557/1557 [00:28<00:00, 53.95it/s, loss=0.044, acc=54.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20, Train Loss: 0.0396, Train Acc: 66.53%, Test Loss: 0.0440, Test Acc: 54.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 6225/6225 [04:07<00:00, 25.15it/s, loss=0.0395, acc=66.6]\n",
            "Epoch 9/20 [Test]: 100%|██████████| 1557/1557 [00:28<00:00, 54.20it/s, loss=0.0411, acc=63.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20, Train Loss: 0.0395, Train Acc: 66.62%, Test Loss: 0.0411, Test Acc: 63.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 6225/6225 [04:08<00:00, 25.00it/s, loss=0.0394, acc=67]\n",
            "Epoch 10/20 [Test]: 100%|██████████| 1557/1557 [00:30<00:00, 51.54it/s, loss=0.0422, acc=57.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20, Train Loss: 0.0394, Train Acc: 67.02%, Test Loss: 0.0422, Test Acc: 57.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 6225/6225 [04:14<00:00, 24.42it/s, loss=0.0392, acc=67.4]\n",
            "Epoch 11/20 [Test]: 100%|██████████| 1557/1557 [00:30<00:00, 51.63it/s, loss=0.0389, acc=68.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20, Train Loss: 0.0392, Train Acc: 67.38%, Test Loss: 0.0389, Test Acc: 68.43%\n",
            "New best model saved with accuracy: 68.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 6225/6225 [04:11<00:00, 24.71it/s, loss=0.0391, acc=67.6]\n",
            "Epoch 12/20 [Test]: 100%|██████████| 1557/1557 [00:28<00:00, 54.39it/s, loss=0.0388, acc=67.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20, Train Loss: 0.0391, Train Acc: 67.58%, Test Loss: 0.0388, Test Acc: 67.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 6225/6225 [04:11<00:00, 24.80it/s, loss=0.0389, acc=67.9]\n",
            "Epoch 13/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 52.60it/s, loss=0.0393, acc=65.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/20, Train Loss: 0.0389, Train Acc: 67.86%, Test Loss: 0.0393, Test Acc: 65.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 6225/6225 [04:13<00:00, 24.59it/s, loss=0.0386, acc=68.6]\n",
            "Epoch 14/20 [Test]: 100%|██████████| 1557/1557 [00:28<00:00, 54.04it/s, loss=0.0432, acc=54.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/20, Train Loss: 0.0386, Train Acc: 68.56%, Test Loss: 0.0432, Test Acc: 54.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 6225/6225 [04:11<00:00, 24.78it/s, loss=0.0382, acc=69.2]\n",
            "Epoch 15/20 [Test]: 100%|██████████| 1557/1557 [00:28<00:00, 55.53it/s, loss=0.0393, acc=66.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/20, Train Loss: 0.0382, Train Acc: 69.20%, Test Loss: 0.0393, Test Acc: 66.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 6225/6225 [04:14<00:00, 24.45it/s, loss=0.0379, acc=69.8]\n",
            "Epoch 16/20 [Test]: 100%|██████████| 1557/1557 [00:28<00:00, 54.06it/s, loss=0.0372, acc=70.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/20, Train Loss: 0.0379, Train Acc: 69.81%, Test Loss: 0.0372, Test Acc: 70.58%\n",
            "New best model saved with accuracy: 70.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 6225/6225 [04:20<00:00, 23.93it/s, loss=0.0375, acc=70.4]\n",
            "Epoch 17/20 [Test]: 100%|██████████| 1557/1557 [00:30<00:00, 50.94it/s, loss=0.0388, acc=68.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/20, Train Loss: 0.0375, Train Acc: 70.38%, Test Loss: 0.0388, Test Acc: 68.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 6225/6225 [04:15<00:00, 24.37it/s, loss=0.0373, acc=70.7]\n",
            "Epoch 18/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 53.11it/s, loss=0.037, acc=70.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20, Train Loss: 0.0373, Train Acc: 70.68%, Test Loss: 0.0370, Test Acc: 70.90%\n",
            "New best model saved with accuracy: 70.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 6225/6225 [04:15<00:00, 24.40it/s, loss=0.0371, acc=70.9]\n",
            "Epoch 19/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 52.64it/s, loss=0.0365, acc=71.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20, Train Loss: 0.0371, Train Acc: 70.94%, Test Loss: 0.0365, Test Acc: 71.52%\n",
            "New best model saved with accuracy: 71.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 6225/6225 [04:14<00:00, 24.42it/s, loss=0.0371, acc=71.1]\n",
            "Epoch 20/20 [Test]: 100%|██████████| 1557/1557 [00:29<00:00, 52.69it/s, loss=0.0364, acc=71.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20, Train Loss: 0.0371, Train Acc: 71.09%, Test Loss: 0.0364, Test Acc: 71.61%\n",
            "New best model saved with accuracy: 71.61%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.67      0.70     49593\n",
            "           1       0.70      0.76      0.73     50007\n",
            "\n",
            "    accuracy                           0.72     99600\n",
            "   macro avg       0.72      0.72      0.72     99600\n",
            "weighted avg       0.72      0.72      0.72     99600\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model accuracy: 71.61%\n",
            "Model saved to: /content/resnetse_pde_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------------\n",
        "# Enhanced Data Augmentation\n",
        "class AugmentationPipeline:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Original tensor shape: C x H x W\n",
        "\n",
        "        # Random horizontal and vertical flips\n",
        "        if random.random() < self.p:\n",
        "            x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "        if random.random() < self.p:\n",
        "            x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "        # Random rotations (90, 180, 270 degrees)\n",
        "        if random.random() < self.p:\n",
        "            k = random.choice([1, 2, 3])  # number of 90-degree rotations\n",
        "            x = torch.rot90(x, k, dims=[1, 2])\n",
        "\n",
        "        # Random brightness and contrast adjustments\n",
        "        if random.random() < self.p:\n",
        "            brightness_factor = random.uniform(0.8, 1.2)\n",
        "            x = x * brightness_factor\n",
        "\n",
        "        # Add Gaussian noise\n",
        "        if random.random() < self.p:\n",
        "            noise = torch.randn_like(x) * 0.05\n",
        "            x = x + noise\n",
        "            x = torch.clamp(x, min=x.min(), max=x.max())  # Keep the range of values\n",
        "\n",
        "        # Random erasing (simulate detector inefficiencies)\n",
        "        if random.random() < self.p:\n",
        "            h, w = x.shape[1], x.shape[2]\n",
        "            area_ratio = random.uniform(0.02, 0.1)\n",
        "            aspect_ratio = random.uniform(0.3, 1.0/0.3)\n",
        "\n",
        "            h_erased = int(round(h * np.sqrt(area_ratio * aspect_ratio)))\n",
        "            w_erased = int(round(w * np.sqrt(area_ratio / aspect_ratio)))\n",
        "\n",
        "            if h_erased < h and w_erased < w:\n",
        "                top = random.randint(0, h - h_erased)\n",
        "                left = random.randint(0, w - w_erased)\n",
        "\n",
        "                mask = torch.ones_like(x)\n",
        "                mask[:, top:top+h_erased, left:left+w_erased] = 0\n",
        "                x = x * mask\n",
        "\n",
        "        return x\n",
        "\n",
        "# --------------------------\n",
        "# Improved Custom Dataset Class with better normalization\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "        self.normalize_per_channel = True  # Channel-wise normalization\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "            print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "            print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "        # Calculate global mean and std per channel for normalization\n",
        "        self.means = []\n",
        "        self.stds = []\n",
        "\n",
        "        if self.normalize_per_channel:\n",
        "            # Assuming data is in (N, H, W, C) format\n",
        "            for c in range(self.data.shape[-1]):\n",
        "                channel_data = self.data[..., c].flatten()\n",
        "                self.means.append(np.mean(channel_data))\n",
        "                self.stds.append(np.std(channel_data) + 1e-6)\n",
        "        else:\n",
        "            # Global normalization\n",
        "            self.means = [np.mean(self.data)]\n",
        "            self.stds = [np.std(self.data) + 1e-6]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply normalization per channel\n",
        "        if self.normalize_per_channel:\n",
        "            # Assuming sample is (H, W, C)\n",
        "            for c in range(sample.shape[-1]):\n",
        "                sample[..., c] = (sample[..., c] - self.means[c]) / self.stds[c]\n",
        "        else:\n",
        "            # Global normalization\n",
        "            sample = (sample - self.means[0]) / self.stds[0]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# --------------------------\n",
        "# Squeeze-and-Excitation Block with improved reduction ratio\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):  # Reduced from 16 to 8 for more capacity\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "# --------------------------\n",
        "# Modified Residual Block with additional attention mechanisms\n",
        "class ModifiedResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ModifiedResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.se = SEBlock(out_channels, reduction=8)\n",
        "\n",
        "        # Add spatial attention\n",
        "        self.spatial_attention = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, 1, kernel_size=7, padding=3, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Channel attention\n",
        "        out = self.se(out)\n",
        "\n",
        "        # Spatial attention\n",
        "        spatial_mask = self.spatial_attention(out)\n",
        "        out = out * spatial_mask\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# --------------------------\n",
        "# Enhanced Physics-Informed Model with deeper architecture\n",
        "class EnhancedPhysicsNet(nn.Module):\n",
        "    def __init__(self, num_classes=2, dropout_rate=0.4):\n",
        "        super(EnhancedPhysicsNet, self).__init__()\n",
        "\n",
        "        # Input Stem - 2 channel input to custom sized model\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(2, 64, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Modified Residual Blocks with attention\n",
        "        self.layer1 = self._make_layer(64, 64, blocks=3, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, blocks=4, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, blocks=6, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, blocks=3, stride=2)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Improved classifier with deeper MLP and higher dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Physics-inspired feature fusion (conserves energy and momentum principles)\n",
        "        self.physics_branch = nn.Sequential(\n",
        "            nn.Conv2d(512, 128, kernel_size=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Linear(512 + 64, 512)\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(ModifiedResidualBlock(in_channels, out_channels, stride=stride))\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ModifiedResidualBlock(out_channels, out_channels, stride=1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        features = x  # Extract convolutional features here for regularization\n",
        "\n",
        "        # Physics branch\n",
        "        physics_features = self.physics_branch(features)\n",
        "\n",
        "        # Main classification branch\n",
        "        x = self.avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Fusion of features\n",
        "        fused = torch.cat([x, physics_features], dim=1)\n",
        "        x = self.fusion(fused)\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# --------------------------\n",
        "# Improved Focal Loss with class balancing\n",
        "class BalancedFocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
        "        super(BalancedFocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha  # Class weights\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        log_softmax = F.log_softmax(inputs, dim=1)\n",
        "        logpt = log_softmax.gather(1, targets.view(-1, 1))\n",
        "        logpt = logpt.view(-1)\n",
        "        pt = logpt.exp()\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.type() != inputs.data.type():\n",
        "                self.alpha = self.alpha.type_as(inputs.data)\n",
        "            at = self.alpha.gather(0, targets.data.view(-1))\n",
        "            logpt = logpt * at\n",
        "\n",
        "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# --------------------------\n",
        "# Enhanced PDE Regularization with multi-scale Laplacian\n",
        "def enhanced_pde_regularization(features):\n",
        "    \"\"\"\n",
        "    Computes a multi-scale PDE-inspired regularization loss\n",
        "    \"\"\"\n",
        "    N, C, H, W = features.shape\n",
        "\n",
        "    # Standard Laplacian kernel\n",
        "    lap_kernel1 = torch.tensor([[0, 1, 0],\n",
        "                               [1, -4, 1],\n",
        "                               [0, 1, 0]], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel1 = lap_kernel1.view(1, 1, 3, 3).repeat(C, 1, 1, 1)\n",
        "\n",
        "    # Larger scale Laplacian kernel (5x5)\n",
        "    lap_kernel2 = torch.tensor([[0, 0, 1, 0, 0],\n",
        "                               [0, 1, 2, 1, 0],\n",
        "                               [1, 2, -16, 2, 1],\n",
        "                               [0, 1, 2, 1, 0],\n",
        "                               [0, 0, 1, 0, 0]], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel2 = lap_kernel2.view(1, 1, 5, 5).repeat(C, 1, 1, 1)\n",
        "\n",
        "    # Apply both kernels\n",
        "    laplacian1 = F.conv2d(features, lap_kernel1, padding=1, groups=C)\n",
        "    laplacian2 = F.conv2d(features, lap_kernel2, padding=2, groups=C)\n",
        "\n",
        "    # Wavelet-inspired multi-scale decomposition\n",
        "    loss_pde = torch.mean(laplacian1 ** 2) + 0.5 * torch.mean(laplacian2 ** 2)\n",
        "\n",
        "    # Add gradient consistency constraint\n",
        "    dx = features[:, :, :, 1:] - features[:, :, :, :-1]\n",
        "    dy = features[:, :, 1:, :] - features[:, :, :-1, :]\n",
        "\n",
        "    grad_reg = torch.mean(dx**2) + torch.mean(dy**2)\n",
        "\n",
        "    return loss_pde + 0.1 * grad_reg\n",
        "\n",
        "# --------------------------\n",
        "# Cross-validation strategy\n",
        "def cross_validation_train(photon_file, electron_file, n_folds=5, num_epochs=40):\n",
        "    # Setup augmentation\n",
        "    transform = AugmentationPipeline(p=0.5)\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = ParticleDataset(photon_file, electron_file, transform=transform)\n",
        "\n",
        "    # Setup cross-validation\n",
        "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
        "\n",
        "    # Track metrics across folds\n",
        "    fold_results = []\n",
        "\n",
        "    # Class distribution to determine weights for balanced sampling\n",
        "    labels = np.array([label for _, label in full_dataset])\n",
        "    class_counts = np.bincount(labels)\n",
        "    class_weights = 1. / class_counts\n",
        "    sample_weights = class_weights[labels]\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(np.zeros(len(full_dataset)), full_dataset.labels)):\n",
        "        print(f'\\n{\"=\"*50}\\nFold {fold+1}/{n_folds}\\n{\"=\"*50}')\n",
        "\n",
        "        # Create samplers for balanced class distribution\n",
        "        train_sampler = WeightedRandomSampler(\n",
        "            weights=sample_weights[train_ids],\n",
        "            num_samples=len(train_ids),\n",
        "            replacement=True\n",
        "        )\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            dataset=torch.utils.data.Subset(full_dataset, train_ids),\n",
        "            batch_size=128,  # Increased batch size\n",
        "            sampler=train_sampler,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            dataset=torch.utils.data.Subset(full_dataset, val_ids),\n",
        "            batch_size=256,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize model, loss, optimizer, and scheduler\n",
        "        model = EnhancedPhysicsNet(num_classes=2).to(device)\n",
        "\n",
        "        # Setup class weights for balanced loss\n",
        "        class_counts = np.bincount(full_dataset.labels[train_ids])\n",
        "        class_weights = torch.FloatTensor(1.0 / (class_counts / len(train_ids))).to(device)\n",
        "\n",
        "        criterion = BalancedFocalLoss(gamma=2.0, alpha=class_weights)\n",
        "\n",
        "        # Use AdamW instead of Adam for better weight decay handling\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "        # Cosine annealing scheduler with warm restarts\n",
        "        scheduler = CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=10,  # Restart every 10 epochs\n",
        "            T_mult=1,\n",
        "            eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        # Mixed precision scaler\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "        # PDE regularization weight\n",
        "        lambda_pde = 0.005  # Reduced from original\n",
        "\n",
        "        # Training loop\n",
        "        best_val_acc = 0.0\n",
        "        train_losses, train_accs = [], []\n",
        "        val_losses, val_accs = [], []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "            for inputs, labels in train_iter:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                    logits, features = model(inputs, return_features=True)\n",
        "                    cls_loss = criterion(logits, labels)\n",
        "                    reg_loss = enhanced_pde_regularization(features)\n",
        "\n",
        "                    # Weighted loss with scheduler-based annealing of regularization\n",
        "                    lambda_factor = lambda_pde * (1.0 - min(epoch / (0.7 * num_epochs), 1.0))\n",
        "                    loss = cls_loss + lambda_factor * reg_loss\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = logits.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "                train_iter.set_postfix({\"loss\": running_loss/total, \"acc\": 100.*correct/total})\n",
        "\n",
        "            # Step the learning rate scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss = running_loss / total\n",
        "            train_acc = 100. * correct / total\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_acc)\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                val_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "                for inputs, labels in val_iter:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                        logits, features = model(inputs, return_features=True)\n",
        "                        cls_loss = criterion(logits, labels)\n",
        "                        reg_loss = enhanced_pde_regularization(features)\n",
        "                        loss = cls_loss + lambda_factor * reg_loss\n",
        "\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = logits.max(1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    # Store predictions and labels for metrics\n",
        "                    all_preds.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                    val_iter.set_postfix({\"loss\": val_loss/total, \"acc\": 100.*correct/total})\n",
        "\n",
        "            val_loss = val_loss / total\n",
        "            val_acc = 100. * correct / total\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), f\"physics_model_fold{fold+1}_best.pth\")\n",
        "                print(f\"New best model saved with accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "        # Final evaluation on validation set\n",
        "        model.load_state_dict(torch.load(f\"physics_model_fold{fold+1}_best.pth\"))\n",
        "        model.eval()\n",
        "\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=\"Final evaluation\"):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                logits = model(inputs)\n",
        "                prob = torch.softmax(logits, dim=1)[:, 1]\n",
        "                all_preds.extend(prob.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate ROC and AUC\n",
        "        fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Calculate binary predictions\n",
        "        y_pred_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
        "\n",
        "        # Calculate confusion matrix\n",
        "        cm = confusion_matrix(all_labels, y_pred_bin)\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(all_labels, y_pred_bin))\n",
        "\n",
        "        # Store fold results\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'roc_auc': roc_auc,\n",
        "            'confusion_matrix': cm\n",
        "        })\n",
        "\n",
        "        # Plot ROC curve for this fold\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'Fold {fold+1} ROC Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig(f'roc_curve_fold{fold+1}.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Photon', 'Electron'],\n",
        "                    yticklabels=['Photon', 'Electron'])\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.ylabel('True label')\n",
        "        plt.title(f'Fold {fold+1} Confusion Matrix')\n",
        "        plt.savefig(f'confusion_matrix_fold{fold+1}.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Plot training curves\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Train Loss')\n",
        "        plt.plot(val_losses, label='Val Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title(f'Fold {fold+1} Loss Curves')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(train_accs, label='Train Accuracy')\n",
        "        plt.plot(val_accs, label='Val Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "        plt.title(f'Fold {fold+1} Accuracy Curves')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'training_curves_fold{fold+1}.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Calculate and display cross-validation results\n",
        "    accuracies = [result['best_val_acc'] for result in fold_results]\n",
        "    aucs = [result['roc_auc'] for result in fold_results]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Cross-validation results over {n_folds} folds:\")\n",
        "    print(f\"Mean accuracy: {np.mean(accuracies):.2f}% ± {np.std(accuracies):.2f}%\")\n",
        "    print(f\"Mean AUC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# --------------------------\n",
        "# Ensemble creation\n",
        "def create_ensemble(photon_file, electron_file, fold_results, n_folds=5):\n",
        "    # Create test dataset without augmentation (for final evaluation)\n",
        "    test_dataset = ParticleDataset(photon_file, electron_file, transform=None)\n",
        "\n",
        "    # Create data loader for testing\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=256,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Load all models for ensemble\n",
        "    ensemble_models = []\n",
        "    for fold in range(n_folds):\n",
        "        model = EnhancedPhysicsNet(num_classes=2).to(device)\n",
        "        model.load_state_dict(torch.load(f\"physics_model_fold{fold+1}_best.pth\"))\n",
        "        model.eval()\n",
        "        ensemble_models.append(model)\n",
        "\n",
        "    # Perform ensemble prediction\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Ensemble prediction\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Get predictions from all models\n",
        "            ensemble_probs = []\n",
        "            for model in ensemble_models:\n",
        "                logits = model(inputs)\n",
        "                probs = torch.softmax(logits, dim=1)[:, 1]\n",
        "                ensemble_probs.append(probs.cpu().numpy())\n",
        "\n",
        "            # Average predictions\n",
        "            avg_probs = np.mean(ensemble_probs, axis=0)\n",
        "            all_preds.exten"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96DCtWQMggk9",
        "outputId": "342c3443-47a7-4c8c-ef38-1822aab8bac4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Based on the provided code, here's an improved version designed to achieve 97-98% accuracy for particle classification:\n",
        "\n",
        "```python\n",
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.base.conv1(x)\n",
        "        x = self.base.bn1(x)\n",
        "        x = self.base.relu(x)\n",
        "\n",
        "        x = self.base.layer1(x)\n",
        "        x = self.base.layer2(x)\n",
        "        x = self.base.layer3(x)\n",
        "        x = self.base.layer4(x)  # Extract convolutional features here\n",
        "\n",
        "        features = x  # Save features for PDE regularization\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc_layers(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# Focal Loss with Label Smoothing\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.25, reduction='mean', label_smoothing=0.1):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Apply label smoothing\n",
        "        num_classes = inputs.size(1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes).float()\n",
        "        targets_smooth = (1 - self.label_smoothing) * targets_one_hot + self.label_smoothing / num_classes\n",
        "\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -torch.sum(targets_smooth * log_probs, dim=1)\n",
        "\n",
        "        # Apply focal weighting\n",
        "        probs = torch.exp(log_probs)\n",
        "        pt = torch.sum(targets_one_hot * probs, dim=1)\n",
        "        focal_weight = self.alpha * ((1 - pt) ** self.gamma)\n",
        "\n",
        "        loss = focal_weight * loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# Enhanced PDE Regularization with multiple physics constraints\n",
        "def enhanced_pde_regularization(features):\n",
        "    \"\"\"\n",
        "    Computes an enhanced PDE-inspired regularization loss by calculating\n",
        "    multiple physics-based constraints on the feature maps.\n",
        "    \"\"\"\n",
        "    N, C, H, W = features.shape\n",
        "\n",
        "    # 1. Laplacian-based smoothness constraint\n",
        "    lap_kernel = torch.tensor([\n",
        "        [0, 1, 0],\n",
        "        [1, -4, 1],\n",
        "        [0, 1, 0]\n",
        "    ], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel = lap_kernel.view(1, 1, 3, 3).repeat(C, 1, 1, 1)\n",
        "    laplacian = F.conv2d(features, lap_kernel, padding=1, groups=C)\n",
        "    loss_laplacian = torch.mean(laplacian ** 2)\n",
        "\n",
        "    # 2. Gradient-based constraint (encourages smooth gradients)\n",
        "    grad_x = features[:, :, :, 1:] - features[:, :, :, :-1]\n",
        "    grad_y = features[:, :, 1:, :] - features[:, :, :-1, :]\n",
        "    loss_grad = torch.mean(grad_x ** 2) + torch.mean(grad_y ** 2)\n",
        "\n",
        "    # 3. Feature correlation constraint (encourages diversity)\n",
        "    features_flat = features.view(N, C, -1)\n",
        "    features_mean = torch.mean(features_flat, dim=2, keepdim=True)\n",
        "    features_centered = features_flat - features_mean\n",
        "    corr_matrix = torch.matmul(features_centered, features_centered.transpose(1, 2))\n",
        "    corr_matrix = corr_matrix / (torch.norm(features_centered, dim=2, keepdim=True) *\n",
        "                                torch.norm(features_centered, dim=2, keepdim=True).transpose(1, 2) + 1e-8)\n",
        "\n",
        "    # We want off-diagonal elements to be small (minimize feature correlation)\n",
        "    mask = 1 - torch.eye(C, device=features.device).unsqueeze(0)\n",
        "    loss_corr = torch.mean(torch.abs(corr_matrix * mask))\n",
        "\n",
        "    # Combine all regularization terms\n",
        "    loss_pde = loss_laplacian + 0.5 * loss_grad + 0.1 * loss_corr\n",
        "\n",
        "    return loss_pde\n",
        "\n",
        "# File paths\n",
        "photon_file = \"/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "electron_file = \"/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "# Download files if they don't exist\n",
        "if not os.path.exists(photon_file) or not os.path.exists(electron_file):\n",
        "    photon_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "    electron_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "    if not os.path.exists(photon_file):\n",
        "        print(f\"Downloading {photon_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(photon_url, photon_file)\n",
        "\n",
        "    if not os.path.exists(electron_file):\n",
        "        print(f\"Downloading {electron_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(electron_url, electron_file)\n",
        "\n",
        "# Create dataset and apply augmentation\n",
        "dataset = ParticleDataset(photon_file, electron_file, transform=advanced_augmentation)\n",
        "\n",
        "# Split dataset: 80% train, 10% validation, 10% test\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(seed)\n",
        ")\n",
        "\n",
        "# DataLoaders with larger batch size\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Initialize Model, Loss, Optimizer, and Scheduler\n",
        "model = EnhancedResNet_PDE(num_classes=2).to(device)\n",
        "print(model)\n",
        "\n",
        "# Focal Loss with label smoothing\n",
        "criterion = FocalLoss(gamma=2, alpha=0.25, label_smoothing=0.1)\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Cosine annealing with warm restarts for better convergence\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "# Mixed precision for faster training\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# Set PDE regularization weight\n",
        "lambda_pde = 0.005\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "\n",
        "# Training Loop with PDE Regularization and Mixup\n",
        "num_epochs = 50  # Increase epochs for better convergence\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "test_losses, test_accs = [], []\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for inputs, labels in train_iter:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Apply mixup with 50% probability\n",
        "        use_mixup = random.random() > 0.5\n",
        "        if use_mixup:\n",
        "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits, features = model(inputs, return_features=True)\n",
        "\n",
        "            if use_mixup:\n",
        "                cls_loss = mixup_criterion(criterion, logits, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                cls_loss = criterion(logits, labels)\n",
        "\n",
        "            reg_loss = enhanced_pde_regularization(features)\n",
        "            loss = cls_loss + lambda_pde * reg_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if not use_mixup:  # Only calculate accuracy for non-mixup batches\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_iter.set_postfix({\"loss\": loss.item(), \"acc\": 100.*correct/max(1, total)})\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / max(1, total)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "        for inputs, labels in val_iter:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda\n",
        "\n",
        "Citations:\n",
        "[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/51473209/11ea836f-019c-4408-8107-95a016b2589e/untitled3.py\n",
        "\n",
        "---\n",
        "Answer from Perplexity: pplx.ai/share"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "-GN4k-lRjvDo",
        "outputId": "442fada3-5568-4795-9fa2-a123483f4d31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 1) (<ipython-input-4-9300d38a15ab>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9300d38a15ab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Based on the provided code, here's an improved version designed to achieve 97-98% accuracy for particle classification:\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.base.conv1(x)\n",
        "        x = self.base.bn1(x)\n",
        "        x = self.base.relu(x)\n",
        "\n",
        "        x = self.base.layer1(x)\n",
        "        x = self.base.layer2(x)\n",
        "        x = self.base.layer3(x)\n",
        "        x = self.base.layer4(x)  # Extract convolutional features here\n",
        "\n",
        "        features = x  # Save features for PDE regularization\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc_layers(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# Focal Loss with Label Smoothing\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.25, reduction='mean', label_smoothing=0.1):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Apply label smoothing\n",
        "        num_classes = inputs.size(1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes).float()\n",
        "        targets_smooth = (1 - self.label_smoothing) * targets_one_hot + self.label_smoothing / num_classes\n",
        "\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -torch.sum(targets_smooth * log_probs, dim=1)\n",
        "\n",
        "        # Apply focal weighting\n",
        "        probs = torch.exp(log_probs)\n",
        "        pt = torch.sum(targets_one_hot * probs, dim=1)\n",
        "        focal_weight = self.alpha * ((1 - pt) ** self.gamma)\n",
        "\n",
        "        loss = focal_weight * loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# Enhanced PDE Regularization with multiple physics constraints\n",
        "def enhanced_pde_regularization(features):\n",
        "    \"\"\"\n",
        "    Computes an enhanced PDE-inspired regularization loss by calculating\n",
        "    multiple physics-based constraints on the feature maps.\n",
        "    \"\"\"\n",
        "    N, C, H, W = features.shape\n",
        "\n",
        "    # 1. Laplacian-based smoothness constraint\n",
        "    lap_kernel = torch.tensor([\n",
        "        [0, 1, 0],\n",
        "        [1, -4, 1],\n",
        "        [0, 1, 0]\n",
        "    ], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel = lap_kernel.view(1, 1, 3, 3).repeat(C, 1, 1, 1)\n",
        "    laplacian = F.conv2d(features, lap_kernel, padding=1, groups=C)\n",
        "    loss_laplacian = torch.mean(laplacian ** 2)\n",
        "\n",
        "    # 2. Gradient-based constraint (encourages smooth gradients)\n",
        "    grad_x = features[:, :, :, 1:] - features[:, :, :, :-1]\n",
        "    grad_y = features[:, :, 1:, :] - features[:, :, :-1, :]\n",
        "    loss_grad = torch.mean(grad_x ** 2) + torch.mean(grad_y ** 2)\n",
        "\n",
        "    # 3. Feature correlation constraint (encourages diversity)\n",
        "    features_flat = features.view(N, C, -1)\n",
        "    features_mean = torch.mean(features_flat, dim=2, keepdim=True)\n",
        "    features_centered = features_flat - features_mean\n",
        "    corr_matrix = torch.matmul(features_centered, features_centered.transpose(1, 2))\n",
        "    corr_matrix = corr_matrix / (torch.norm(features_centered, dim=2, keepdim=True) *\n",
        "                                torch.norm(features_centered, dim=2, keepdim=True).transpose(1, 2) + 1e-8)\n",
        "\n",
        "    # We want off-diagonal elements to be small (minimize feature correlation)\n",
        "    mask = 1 - torch.eye(C, device=features.device).unsqueeze(0)\n",
        "    loss_corr = torch.mean(torch.abs(corr_matrix * mask))\n",
        "\n",
        "    # Combine all regularization terms\n",
        "    loss_pde = loss_laplacian + 0.5 * loss_grad + 0.1 * loss_corr\n",
        "\n",
        "    return loss_pde\n",
        "\n",
        "# File paths\n",
        "photon_file = \"/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "electron_file = \"/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "# Download files if they don't exist\n",
        "if not os.path.exists(photon_file) or not os.path.exists(electron_file):\n",
        "    photon_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "    electron_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "    if not os.path.exists(photon_file):\n",
        "        print(f\"Downloading {photon_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(photon_url, photon_file)\n",
        "\n",
        "    if not os.path.exists(electron_file):\n",
        "        print(f\"Downloading {electron_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(electron_url, electron_file)\n",
        "\n",
        "# Create dataset and apply augmentation\n",
        "dataset = ParticleDataset(photon_file, electron_file, transform=advanced_augmentation)\n",
        "\n",
        "# Split dataset: 80% train, 10% validation, 10% test\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(seed)\n",
        ")\n",
        "\n",
        "# DataLoaders with larger batch size\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Initialize Model, Loss, Optimizer, and Scheduler\n",
        "model = EnhancedResNet_PDE(num_classes=2).to(device)\n",
        "print(model)\n",
        "\n",
        "# Focal Loss with label smoothing\n",
        "criterion = FocalLoss(gamma=2, alpha=0.25, label_smoothing=0.1)\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Cosine annealing with warm restarts for better convergence\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "# Mixed precision for faster training\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# Set PDE regularization weight\n",
        "lambda_pde = 0.005\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "\n",
        "# Training Loop with PDE Regularization and Mixup\n",
        "num_epochs = 50  # Increase epochs for better convergence\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "test_losses, test_accs = [], []\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for inputs, labels in train_iter:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Apply mixup with 50% probability\n",
        "        use_mixup = random.random() > 0.5\n",
        "        if use_mixup:\n",
        "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits, features = model(inputs, return_features=True)\n",
        "\n",
        "            if use_mixup:\n",
        "                cls_loss = mixup_criterion(criterion, logits, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                cls_loss = criterion(logits, labels)\n",
        "\n",
        "            reg_loss = enhanced_pde_regularization(features)\n",
        "            loss = cls_loss + lambda_pde * reg_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if not use_mixup:  # Only calculate accuracy for non-mixup batches\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_iter.set_postfix({\"loss\": loss.item(), \"acc\": 100.*correct/max(1, total)})\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / max(1, total)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "        for inputs, labels in val_iter:\n",
        "```python\n",
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "yHV2YLG_j7OF",
        "outputId": "4f505970-1ffd-4e25-d61a-6e2b6ebfecd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 409 (<ipython-input-7-c601038bcb8b>, line 410)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-c601038bcb8b>\"\u001b[0;36m, line \u001b[0;32m410\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Squeeze-and-Excitation (SE) Block\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n"
      ],
      "metadata": {
        "id": "RxrjaaN2kpSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.base.conv1(x)\n",
        "        x = self.base.bn1(x)\n",
        "        x = self.base.relu(x)\n",
        "\n",
        "        x = self.base.layer1(x)\n",
        "        x = self.base.layer2(x)\n",
        "        x = self.base.layer3(x)\n",
        "        x = self.base.layer4(x)  # Extract convolutional features here\n",
        "\n",
        "        features = x  # Save features for PDE regularization\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc_layers(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# Focal Loss with Label Smoothing\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.25, reduction='mean', label_smoothing=0.1):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Apply label smoothing\n",
        "        num_classes = inputs.size(1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes).float()\n",
        "        targets_smooth = (1 - self.label_smoothing) * targets_one_hot + self.label_smoothing / num_classes\n",
        "\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -torch.sum(targets_smooth * log_probs, dim=1)\n",
        "\n",
        "        # Apply focal weighting\n",
        "        probs = torch.exp(log_probs)\n",
        "        pt = torch.sum(targets_one_hot * probs, dim=1)\n",
        "        focal_weight = self.alpha * ((1 - pt) ** self.gamma)\n",
        "\n",
        "        loss = focal_weight * loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# Enhanced PDE Regularization with multiple physics constraints\n",
        "def enhanced_pde_regularization(features):\n",
        "    \"\"\"\n",
        "    Computes an enhanced PDE-inspired regularization loss by calculating\n",
        "    multiple physics-based constraints on the feature maps.\n",
        "    \"\"\"\n",
        "    N, C, H, W = features.shape\n",
        "\n",
        "    # 1. Laplacian-based smoothness constraint\n",
        "    lap_kernel = torch.tensor([\n",
        "        [0, 1, 0],\n",
        "        [1, -4, 1],\n",
        "        [0, 1, 0]\n",
        "    ], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel = lap_kernel.view(1, 1, 3, 3).repeat(C, 1, 1, 1)\n",
        "    laplacian = F.conv2d(features, lap_kernel, padding=1, groups=C)\n",
        "    loss_laplacian = torch.mean(laplacian ** 2)\n",
        "\n",
        "    # 2. Gradient-based constraint (encourages smooth gradients)\n",
        "    grad_x = features[:, :, :, 1:] - features[:, :, :, :-1]\n",
        "    grad_y = features[:, :, 1:, :] - features[:, :, :-1, :]\n",
        "    loss_grad = torch.mean(grad_x ** 2) + torch.mean(grad_y ** 2)\n",
        "\n",
        "    # 3. Feature correlation constraint (encourages diversity)\n",
        "    features_flat = features.view(N, C, -1)\n",
        "    features_mean = torch.mean(features_flat, dim=2, keepdim=True)\n",
        "    features_centered = features_flat - features_mean\n",
        "    corr_matrix = torch.matmul(features_centered, features_centered.transpose(1, 2))\n",
        "    corr_matrix = corr_matrix / (torch.norm(features_centered, dim=2, keepdim=True) *\n",
        "                                torch.norm(features_centered, dim=2, keepdim=True).transpose(1, 2) + 1e-8)\n",
        "\n",
        "    # We want off-diagonal elements to be small (minimize feature correlation)\n",
        "    mask = 1 - torch.eye(C, device=features.device).unsqueeze(0)\n",
        "    loss_corr = torch.mean(torch.abs(corr_matrix * mask))\n",
        "\n",
        "    # Combine all regularization terms\n",
        "    loss_pde = loss_laplacian + 0.5 * loss_grad + 0.1 * loss_corr\n",
        "\n",
        "    return loss_pde\n",
        "\n",
        "# File paths\n",
        "photon_file = \"/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "electron_file = \"/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "# Download files if they don't exist\n",
        "if not os.path.exists(photon_file) or not os.path.exists(electron_file):\n",
        "    photon_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "    electron_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "    if not os.path.exists(photon_file):\n",
        "        print(f\"Downloading {photon_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(photon_url, photon_file)\n",
        "\n",
        "    if not os.path.exists(electron_file):\n",
        "        print(f\"Downloading {electron_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(electron_url, electron_file)\n",
        "\n",
        "# Create dataset and apply augmentation\n",
        "dataset = ParticleDataset(photon_file, electron_file, transform=advanced_augmentation)\n",
        "\n",
        "# Split dataset: 80% train, 10% validation, 10% test\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(seed)\n",
        ")\n",
        "\n",
        "# DataLoaders with larger batch size\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Initialize Model, Loss, Optimizer, and Scheduler\n",
        "model = EnhancedResNet_PDE(num_classes=2).to(device)\n",
        "print(model)\n",
        "\n",
        "# Focal Loss with label smoothing\n",
        "criterion = FocalLoss(gamma=2, alpha=0.25, label_smoothing=0.1)\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Cosine annealing with warm restarts for better convergence\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "# Mixed precision for faster training\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# Set PDE regularization weight\n",
        "lambda_pde = 0.005\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "\n",
        "# Training Loop with PDE Regularization and Mixup\n",
        "num_epochs = 50  # Increase epochs for better convergence\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "test_losses, test_accs = [], []\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for inputs, labels in train_iter:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Apply mixup with 50% probability\n",
        "        use_mixup = random.random() > 0.5\n",
        "        if use_mixup:\n",
        "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits, features = model(inputs, return_features=True)\n",
        "\n",
        "            if use_mixup:\n",
        "                cls_loss = mixup_criterion(criterion, logits, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                cls_loss = criterion(logits, labels)\n",
        "\n",
        "            reg_loss = enhanced_pde_regularization(features)\n",
        "            loss = cls_loss + lambda_pde * reg_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if not use_mixup:  # Only calculate accuracy for non-mixup batches\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_iter.set_postfix({\"loss\": loss.item(), \"acc\": 100.*correct/max(1, total)})\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / max(1, total)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "        for inputs, labels in val_iter:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# ... (rest of the code remains unchanged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "LslZZMbklHh2",
        "outputId": "8748435a-2417-4580-9646-c7a3a10fbe94"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-8-c8a6e8ac4327>, line 413)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-c8a6e8ac4327>\"\u001b[0;36m, line \u001b[0;32m413\u001b[0m\n\u001b[0;31m    logits, features = model(inputs\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.base.conv1(x)\n",
        "        x = self.base.bn1(x)\n",
        "        x = self.base.relu(x)\n",
        "\n",
        "        x = self.base.layer1(x)\n",
        "        x = self.base.layer2(x)\n",
        "        x = self.base.layer3(x)\n",
        "        x = self.base.layer4(x)  # Extract convolutional features here\n",
        "\n",
        "        features = x  # Save features for PDE regularization\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc_layers(x)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# Focal Loss with Label Smoothing\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.25, reduction='mean', label_smoothing=0.1):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Apply label smoothing\n",
        "        num_classes = inputs.size(1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes).float()\n",
        "        targets_smooth = (1 - self.label_smoothing) * targets_one_hot + self.label_smoothing / num_classes\n",
        "\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        loss = -torch.sum(targets_smooth * log_probs, dim=1)\n",
        "\n",
        "        # Apply focal weighting\n",
        "        probs = torch.exp(log_probs)\n",
        "        pt = torch.sum(targets_one_hot * probs, dim=1)\n",
        "        focal_weight = self.alpha * ((1 - pt) ** self.gamma)\n",
        "\n",
        "        loss = focal_weight * loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# Enhanced PDE Regularization with multiple physics constraints\n",
        "def enhanced_pde_regularization(features):\n",
        "    \"\"\"\n",
        "    Computes an enhanced PDE-inspired regularization loss by calculating\n",
        "    multiple physics-based constraints on the feature maps.\n",
        "    \"\"\"\n",
        "    N, C, H, W = features.shape\n",
        "\n",
        "    # 1. Laplacian-based smoothness constraint\n",
        "    lap_kernel = torch.tensor([\n",
        "        [0, 1, 0],\n",
        "        [1, -4, 1],\n",
        "        [0, 1, 0]\n",
        "    ], dtype=torch.float32, device=features.device)\n",
        "    lap_kernel = lap_kernel.view(1, 1, 3, 3).repeat(C, 1, 1, 1)\n",
        "    laplacian = F.conv2d(features, lap_kernel, padding=1, groups=C)\n",
        "    loss_laplacian = torch.mean(laplacian ** 2)\n",
        "\n",
        "    # 2. Gradient-based constraint (encourages smooth gradients)\n",
        "    grad_x = features[:, :, :, 1:] - features[:, :, :, :-1]\n",
        "    grad_y = features[:, :, 1:, :] - features[:, :, :-1, :]\n",
        "    loss_grad = torch.mean(grad_x ** 2) + torch.mean(grad_y ** 2)\n",
        "\n",
        "    # 3. Feature correlation constraint (encourages diversity)\n",
        "    features_flat = features.view(N, C, -1)\n",
        "    features_mean = torch.mean(features_flat, dim=2, keepdim=True)\n",
        "    features_centered = features_flat - features_mean\n",
        "    corr_matrix = torch.matmul(features_centered, features_centered.transpose(1, 2))\n",
        "    corr_matrix = corr_matrix / (torch.norm(features_centered, dim=2, keepdim=True) *\n",
        "                                torch.norm(features_centered, dim=2, keepdim=True).transpose(1, 2) + 1e-8)\n",
        "\n",
        "    # We want off-diagonal elements to be small (minimize feature correlation)\n",
        "    mask = 1 - torch.eye(C, device=features.device).unsqueeze(0)\n",
        "    loss_corr = torch.mean(torch.abs(corr_matrix * mask))\n",
        "\n",
        "    # Combine all regularization terms\n",
        "    loss_pde = loss_laplacian + 0.5 * loss_grad + 0.1 * loss_corr\n",
        "\n",
        "    return loss_pde\n",
        "\n",
        "# File paths\n",
        "photon_file = \"/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "electron_file = \"/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "# Download files if they don't exist\n",
        "if not os.path.exists(photon_file) or not os.path.exists(electron_file):\n",
        "    photon_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/AtBT8y4MiQYFcgc/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "    electron_url = \"https://cernbox.cern.ch/remote.php/dav/public-files/FbXw3V4XNyYB3oA/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "\n",
        "    if not os.path.exists(photon_file):\n",
        "        print(f\"Downloading {photon_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(photon_url, photon_file)\n",
        "\n",
        "    if not os.path.exists(electron_file):\n",
        "        print(f\"Downloading {electron_file} ...\")\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(electron_url, electron_file)\n",
        "\n",
        "# Create dataset and apply augmentation\n",
        "dataset = ParticleDataset(photon_file, electron_file, transform=advanced_augmentation)\n",
        "\n",
        "# Split dataset: 80% train, 10% validation, 10% test\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(seed)\n",
        ")\n",
        "\n",
        "# DataLoaders with larger batch size\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Initialize Model, Loss, Optimizer, and Scheduler\n",
        "model = EnhancedResNet_PDE(num_classes=2).to(device)\n",
        "print(model)\n",
        "\n",
        "# Focal Loss with label smoothing\n",
        "criterion = FocalLoss(gamma=2, alpha=0.25, label_smoothing=0.1)\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Cosine annealing with warm restarts for better convergence\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "# Mixed precision for faster training\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# Set PDE regularization weight\n",
        "lambda_pde = 0.005\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "\n",
        "# Training Loop with PDE Regularization and Mixup\n",
        "num_epochs = 50  # Increase epochs for better convergence\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "test_losses, test_accs = [], []\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for inputs, labels in train_iter:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Apply mixup with 50% probability\n",
        "        use_mixup = random.random() > 0.5\n",
        "        if use_mixup:\n",
        "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits, features = model(inputs, return_features=True)\n",
        "\n",
        "            if use_mixup:\n",
        "                cls_loss = mixup_criterion(criterion, logits, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                cls_loss = criterion(logits, labels)\n",
        "\n",
        "            reg_loss = enhanced_pde_regularization(features)\n",
        "            loss = cls_loss + lambda_pde * reg_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if not use_mixup:  # Only calculate accuracy for non-mixup batches\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_iter.set_postfix({\"loss\": loss.item(), \"acc\": 100.*correct/max(1, total)})\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / max(1, total)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for inputs, labels in train_iter:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Apply mixup with 50% probability\n",
        "        use_mixup = random.random() > 0.5\n",
        "        if use_mixup:\n",
        "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits, features = model(inputs, return_features=True)\n",
        "\n",
        "            if use_mixup:\n",
        "                cls_loss = mixup_criterion(criterion, logits, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                cls_loss = criterion(logits, labels)\n",
        "\n",
        "            reg_loss = enhanced_pde_regularization(features)\n",
        "            loss = cls_loss + lambda_pde * reg_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if not use_mixup:  # Only calculate accuracy for non-mixup batches\n",
        "            _, predicted = logits.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_iter.set_postfix({\"loss\": loss.item(), \"acc\": 100.*correct/max(1, total)})\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / max(1, total)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "```python\n",
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "def advanced_augmentation(x):\n",
        "    # Basic flips\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[2])  # horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        x = torch.flip(x, dims=[1])  # vertical flip\n",
        "\n",
        "    # Add random rotation\n",
        "    if random.random() > 0.7:\n",
        "        angle = random.uniform(-15, 15)\n",
        "        x = TF.rotate(x, angle)\n",
        "\n",
        "    # Add random noise\n",
        "    if random.random() > 0.7:\n",
        "        noise = torch.randn_like(x) * 0.02\n",
        "        x = x + noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Custom Dataset Class to load Particle data\n",
        "class ParticleDataset(Dataset):\n",
        "    def __init__(self, photon_file, electron_file, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read photon dataset\n",
        "        with h5py.File(photon_file, 'r') as f_photon:\n",
        "            key = list(f_photon.keys())[0]\n",
        "            self.photon_data = f_photon[key][:]\n",
        "            self.photon_labels = np.zeros(self.photon_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded photon data: {self.photon_data.shape}\")\n",
        "\n",
        "        # Read electron dataset\n",
        "        with h5py.File(electron_file, 'r') as f_electron:\n",
        "            key = list(f_electron.keys())[0]\n",
        "            self.electron_data = f_electron[key][:]\n",
        "            self.electron_labels = np.ones(self.electron_data.shape[0], dtype=np.int64)\n",
        "        print(f\"Loaded electron data: {self.electron_data.shape}\")\n",
        "\n",
        "        # Concatenate datasets\n",
        "        self.data = np.concatenate((self.photon_data, self.electron_data), axis=0)\n",
        "        self.labels = np.concatenate((self.photon_labels, self.electron_labels), axis=0)\n",
        "        print(f\"Total samples: {self.data.shape[0]}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert sample to float32 and apply robust normalization\n",
        "        sample = self.data[idx].astype(np.float32)\n",
        "\n",
        "        # Robust normalization with clipping\n",
        "        mean = np.mean(sample)\n",
        "        std = np.std(sample)\n",
        "        sample = np.clip((sample - mean) / (std + 1e-5), -5, 5)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensor (assumes shape: H x W x C; convert to C x H x W)\n",
        "        sample = torch.tensor(sample)\n",
        "        if sample.ndim == 3:\n",
        "            sample = sample.permute(2, 0, 1)\n",
        "\n",
        "        # Apply augmentation if any\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, torch.tensor(label)\n",
        "\n",
        "# Channel Attention Module\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention Module\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# CBAM Module\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x) * x\n",
        "        x = self.spatial_att(x) * x\n",
        "        return x\n",
        "\n",
        "# Build an Enhanced Physics-Informed Model with ResNet50 and CBAM\n",
        "class EnhancedResNet_PDE(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedResNet_PDE, self).__init__()\n",
        "\n",
        "        # Load ResNet50 backbone for better feature extraction\n",
        "        self.base = resnet50(pretrained=True)\n",
        "\n",
        "        # Modify first convolution to accept 2-channel input\n",
        "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the initial max pool (preserving resolution for 32x32 images)\n",
        "        self.base.maxpool = nn.Identity()\n",
        "\n",
        "        # Inject CBAM blocks in each layer group\n",
        "        self.base.layer1[0] = nn.Sequential(self.base.layer1[0], CBAM(256))\n",
        "        self.base.layer2[0] = nn.Sequential(self.base.layer2[0], CBAM(512))\n",
        "        self.base.layer3[0] = nn.Sequential(self.base.layer3[0], CBAM(1024))\n",
        "        self.base.layer4[0] = nn.Sequential(self.base.layer4[0], CBAM(2048))\n",
        "\n",
        "        # Additional fully connected layers with dropout for regularization\n",
        "        self.avgpool = self.base.avgpool\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "uNpxA0QAlYPZ",
        "outputId": "a7dcf1fd-0b93-43cc-ea34-81ae68220492"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-26-be37f4bb84cc>, line 614)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-be37f4bb84cc>\"\u001b[0;36m, line \u001b[0;32m614\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}